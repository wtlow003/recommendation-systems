{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 0. Import packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\nINFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from pandarallel import pandarallel\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from src.data import load_dataset\n",
    "from src.features import build_features, preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "pandarallel.initialize()\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "source": [
    "### 1. Loading processed data pickle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "DATA_PATH = Path('data/processed/')\n",
    "CATEGORY = 'Clothing_Shoes_and_Jewelry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"{DATA_PATH}/{CATEGORY}_train.csv\")\n",
    "test = pd.read_csv(f\"{DATA_PATH}/{CATEGORY}_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train: (231491, 6)\nTest: (47135, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         index  overall      reviewerID        asin  \\\n",
       "0            0      5.0  A1KLRMWW2FWPL4  0000031887   \n",
       "1            1      5.0  A2G5TCU2WDFZ65  0000031887   \n",
       "2            2      5.0  A1RLQXYNCMWRWN  0000031887   \n",
       "3            3      5.0   A8U3FAMSJVHS5  0000031887   \n",
       "4            4      5.0  A3GEOILWLK86XM  0000031887   \n",
       "231486  278629      5.0   AEL6CQNQXONBX  B00KKXCJQU   \n",
       "231487  278630      5.0   ACJT8MUC0LRF0  B00KKXCJQU   \n",
       "231488  278632      5.0  A1EVV74UQYVKRY  B00KKXCJQU   \n",
       "231489  278633      5.0  A1UQBFCERIP7VJ  B00KKXCJQU   \n",
       "231490  278635      5.0  A30VWT3R25QAVD  B00KKXCJQU   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       This is a great tutu and at a really great pri...   \n",
       "1       I bought this for my 4 yr old daughter for dan...   \n",
       "2       What can I say... my daughters have it in oran...   \n",
       "3       We bought several tutus at once, and they are ...   \n",
       "4       Thank you Halo Heaven great product for Little...   \n",
       "231486  This set of travel organizers includes four pi...   \n",
       "231487  When I pack it looks like a disaster area in a...   \n",
       "231488  I've been traveling back and forth to England ...   \n",
       "231489  These are very nice packing cubes and the 18 x...   \n",
       "231490  When I signed up to receive a free set of Shac...   \n",
       "\n",
       "                                      processedReviewText  \n",
       "0       this great tutu great price it look cheap glad...  \n",
       "1       buy yr old daughter dance class wore today tim...  \n",
       "2       what daughters orange black white pink think b...  \n",
       "3       we buy tutu get high review sturdy seemingly t...  \n",
       "4       thank halo heaven great product little girls m...  \n",
       "231486  this set travel organizer include piece total ...  \n",
       "231487  when pack look like disaster area suitcase pac...  \n",
       "231488  travel forth england pack way suitcases some p...  \n",
       "231489  these nice packing cube laundry storage bag ni...  \n",
       "231490  when sign receive free set shacke pak review t...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>overall</th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewText</th>\n      <th>processedReviewText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>5.0</td>\n      <td>A1KLRMWW2FWPL4</td>\n      <td>0000031887</td>\n      <td>This is a great tutu and at a really great pri...</td>\n      <td>this great tutu great price it look cheap glad...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>5.0</td>\n      <td>A2G5TCU2WDFZ65</td>\n      <td>0000031887</td>\n      <td>I bought this for my 4 yr old daughter for dan...</td>\n      <td>buy yr old daughter dance class wore today tim...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5.0</td>\n      <td>A1RLQXYNCMWRWN</td>\n      <td>0000031887</td>\n      <td>What can I say... my daughters have it in oran...</td>\n      <td>what daughters orange black white pink think b...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>5.0</td>\n      <td>A8U3FAMSJVHS5</td>\n      <td>0000031887</td>\n      <td>We bought several tutus at once, and they are ...</td>\n      <td>we buy tutu get high review sturdy seemingly t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5.0</td>\n      <td>A3GEOILWLK86XM</td>\n      <td>0000031887</td>\n      <td>Thank you Halo Heaven great product for Little...</td>\n      <td>thank halo heaven great product little girls m...</td>\n    </tr>\n    <tr>\n      <th>231486</th>\n      <td>278629</td>\n      <td>5.0</td>\n      <td>AEL6CQNQXONBX</td>\n      <td>B00KKXCJQU</td>\n      <td>This set of travel organizers includes four pi...</td>\n      <td>this set travel organizer include piece total ...</td>\n    </tr>\n    <tr>\n      <th>231487</th>\n      <td>278630</td>\n      <td>5.0</td>\n      <td>ACJT8MUC0LRF0</td>\n      <td>B00KKXCJQU</td>\n      <td>When I pack it looks like a disaster area in a...</td>\n      <td>when pack look like disaster area suitcase pac...</td>\n    </tr>\n    <tr>\n      <th>231488</th>\n      <td>278632</td>\n      <td>5.0</td>\n      <td>A1EVV74UQYVKRY</td>\n      <td>B00KKXCJQU</td>\n      <td>I've been traveling back and forth to England ...</td>\n      <td>travel forth england pack way suitcases some p...</td>\n    </tr>\n    <tr>\n      <th>231489</th>\n      <td>278633</td>\n      <td>5.0</td>\n      <td>A1UQBFCERIP7VJ</td>\n      <td>B00KKXCJQU</td>\n      <td>These are very nice packing cubes and the 18 x...</td>\n      <td>these nice packing cube laundry storage bag ni...</td>\n    </tr>\n    <tr>\n      <th>231490</th>\n      <td>278635</td>\n      <td>5.0</td>\n      <td>A30VWT3R25QAVD</td>\n      <td>B00KKXCJQU</td>\n      <td>When I signed up to receive a free set of Shac...</td>\n      <td>when sign receive free set shacke pak review t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# check train\n",
    "train.head().append(train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        index  overall      reviewerID        asin  \\\n",
       "0          11      5.0  A2A2WZYLU528RO  0000031887   \n",
       "1          16      5.0  A1MXJVYXE2QU6H  0000031887   \n",
       "2          41      5.0  A35V32HZEGZH04  1608299953   \n",
       "3          44      5.0  A216NSW58Q3SCJ  1617160377   \n",
       "4          47      5.0   AY3D7DG5L5WCK  1617160377   \n",
       "47130  278605      5.0  A3VE6Q3RY2JIM1  B00KA602SY   \n",
       "47131  278607      2.0  A2FENE35P9Z592  B00KCWMG5S   \n",
       "47132  278625      5.0   ACJT8MUC0LRF0  B00KGCLROK   \n",
       "47133  278631      5.0  A2DG63DN704LOI  B00KKXCJQU   \n",
       "47134  278634      5.0  A22CW0ZHY3NJH8  B00KKXCJQU   \n",
       "\n",
       "                                              reviewText  \\\n",
       "0      My daughter has worn this skirt almost every d...   \n",
       "1      Perfect for my budding grand daughter ballerin...   \n",
       "2      .When in the military,  I was stationed in Eur...   \n",
       "3      Although I haven't kept up with the lessons as...   \n",
       "4      I love Rosetta Stone it is easy to use in a fe...   \n",
       "47130  Much better quality and texture than expected ...   \n",
       "47131                                For tiny teenagers.   \n",
       "47132  You don&#8217;t have to be a yoga or martial a...   \n",
       "47133  I don't normally go ga-ga over a product very ...   \n",
       "47134  I am on vacation with my family of four and th...   \n",
       "\n",
       "                                     processedReviewText  \n",
       "0      my daughter worn skirt day receive washer clot...  \n",
       "1      perfect bud grand daughter ballerina beautiful...  \n",
       "2      when military station europe year the friend q...  \n",
       "3      although keep lesson learn lot italian level s...  \n",
       "4      love rosetta stone easy use quick lesson speak...  \n",
       "47130  much good quality texture expect price the col...  \n",
       "47131                                  for tiny teenager  \n",
       "47132  you yoga martial art practitioner enjoy half t...  \n",
       "47133  normally ga ga product cub awesome help review...  \n",
       "47134  vacation family shacke pak set wonderful excep...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>overall</th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewText</th>\n      <th>processedReviewText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>5.0</td>\n      <td>A2A2WZYLU528RO</td>\n      <td>0000031887</td>\n      <td>My daughter has worn this skirt almost every d...</td>\n      <td>my daughter worn skirt day receive washer clot...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16</td>\n      <td>5.0</td>\n      <td>A1MXJVYXE2QU6H</td>\n      <td>0000031887</td>\n      <td>Perfect for my budding grand daughter ballerin...</td>\n      <td>perfect bud grand daughter ballerina beautiful...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>5.0</td>\n      <td>A35V32HZEGZH04</td>\n      <td>1608299953</td>\n      <td>.When in the military,  I was stationed in Eur...</td>\n      <td>when military station europe year the friend q...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44</td>\n      <td>5.0</td>\n      <td>A216NSW58Q3SCJ</td>\n      <td>1617160377</td>\n      <td>Although I haven't kept up with the lessons as...</td>\n      <td>although keep lesson learn lot italian level s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>47</td>\n      <td>5.0</td>\n      <td>AY3D7DG5L5WCK</td>\n      <td>1617160377</td>\n      <td>I love Rosetta Stone it is easy to use in a fe...</td>\n      <td>love rosetta stone easy use quick lesson speak...</td>\n    </tr>\n    <tr>\n      <th>47130</th>\n      <td>278605</td>\n      <td>5.0</td>\n      <td>A3VE6Q3RY2JIM1</td>\n      <td>B00KA602SY</td>\n      <td>Much better quality and texture than expected ...</td>\n      <td>much good quality texture expect price the col...</td>\n    </tr>\n    <tr>\n      <th>47131</th>\n      <td>278607</td>\n      <td>2.0</td>\n      <td>A2FENE35P9Z592</td>\n      <td>B00KCWMG5S</td>\n      <td>For tiny teenagers.</td>\n      <td>for tiny teenager</td>\n    </tr>\n    <tr>\n      <th>47132</th>\n      <td>278625</td>\n      <td>5.0</td>\n      <td>ACJT8MUC0LRF0</td>\n      <td>B00KGCLROK</td>\n      <td>You don&amp;#8217;t have to be a yoga or martial a...</td>\n      <td>you yoga martial art practitioner enjoy half t...</td>\n    </tr>\n    <tr>\n      <th>47133</th>\n      <td>278631</td>\n      <td>5.0</td>\n      <td>A2DG63DN704LOI</td>\n      <td>B00KKXCJQU</td>\n      <td>I don't normally go ga-ga over a product very ...</td>\n      <td>normally ga ga product cub awesome help review...</td>\n    </tr>\n    <tr>\n      <th>47134</th>\n      <td>278634</td>\n      <td>5.0</td>\n      <td>A22CW0ZHY3NJH8</td>\n      <td>B00KKXCJQU</td>\n      <td>I am on vacation with my family of four and th...</td>\n      <td>vacation family shacke pak set wonderful excep...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# check test\n",
    "test.head().append(test.tail())"
   ]
  },
  {
   "source": [
    "### 2. Doc2Vec model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1.42 s, sys: 406 ms, total: 1.82 s\nWall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenising review text\n",
    "train['tokenizedReviewText'] = train['processedReviewText'].parallel_apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index  overall      reviewerID        asin  \\\n",
       "0      0      5.0  A1KLRMWW2FWPL4  0000031887   \n",
       "1      1      5.0  A2G5TCU2WDFZ65  0000031887   \n",
       "2      2      5.0  A1RLQXYNCMWRWN  0000031887   \n",
       "3      3      5.0   A8U3FAMSJVHS5  0000031887   \n",
       "4      4      5.0  A3GEOILWLK86XM  0000031887   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  This is a great tutu and at a really great pri...   \n",
       "1  I bought this for my 4 yr old daughter for dan...   \n",
       "2  What can I say... my daughters have it in oran...   \n",
       "3  We bought several tutus at once, and they are ...   \n",
       "4  Thank you Halo Heaven great product for Little...   \n",
       "\n",
       "                                 processedReviewText  \\\n",
       "0  this great tutu great price it look cheap glad...   \n",
       "1  buy yr old daughter dance class wore today tim...   \n",
       "2  what daughters orange black white pink think b...   \n",
       "3  we buy tutu get high review sturdy seemingly t...   \n",
       "4  thank halo heaven great product little girls m...   \n",
       "\n",
       "                                 tokenizedReviewText  \n",
       "0  [this, great, tutu, great, price, it, look, ch...  \n",
       "1  [buy, yr, old, daughter, dance, class, wore, t...  \n",
       "2  [what, daughters, orange, black, white, pink, ...  \n",
       "3  [we, buy, tutu, get, high, review, sturdy, see...  \n",
       "4  [thank, halo, heaven, great, product, little, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>overall</th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewText</th>\n      <th>processedReviewText</th>\n      <th>tokenizedReviewText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>5.0</td>\n      <td>A1KLRMWW2FWPL4</td>\n      <td>0000031887</td>\n      <td>This is a great tutu and at a really great pri...</td>\n      <td>this great tutu great price it look cheap glad...</td>\n      <td>[this, great, tutu, great, price, it, look, ch...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>5.0</td>\n      <td>A2G5TCU2WDFZ65</td>\n      <td>0000031887</td>\n      <td>I bought this for my 4 yr old daughter for dan...</td>\n      <td>buy yr old daughter dance class wore today tim...</td>\n      <td>[buy, yr, old, daughter, dance, class, wore, t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5.0</td>\n      <td>A1RLQXYNCMWRWN</td>\n      <td>0000031887</td>\n      <td>What can I say... my daughters have it in oran...</td>\n      <td>what daughters orange black white pink think b...</td>\n      <td>[what, daughters, orange, black, white, pink, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>5.0</td>\n      <td>A8U3FAMSJVHS5</td>\n      <td>0000031887</td>\n      <td>We bought several tutus at once, and they are ...</td>\n      <td>we buy tutu get high review sturdy seemingly t...</td>\n      <td>[we, buy, tutu, get, high, review, sturdy, see...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5.0</td>\n      <td>A3GEOILWLK86XM</td>\n      <td>0000031887</td>\n      <td>Thank you Halo Heaven great product for Little...</td>\n      <td>thank halo heaven great product little girls m...</td>\n      <td>[thank, halo, heaven, great, product, little, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# check `tokenizedReviewText`\n",
    "train.head()"
   ]
  },
  {
   "source": [
    "#### 2.1 Preparing `user` and `item` tagged documents"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_documents, asin_documents = preprocessing.prepare_tagged_documents(users='reviewerID', asins='asin', reviews='tokenizedReviewText', df=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[TaggedDocument(words=['this', 'great', 'tutu', 'great', 'price', 'it', 'look', 'cheap', 'glad', 'look', 'amazon', 'affordable', 'tutu', 'poorly'], tags=['A1KLRMWW2FWPL4']),\n TaggedDocument(words=['buy', 'yr', 'old', 'daughter', 'dance', 'class', 'wore', 'today', 'time', 'teacher', 'think', 'adorable', 'buy', 'light', 'blue', 'long', 'sleeve', 'leotard', 'happy', 'color', 'match', 'great', 'price', 'good', 'dollar'], tags=['A2G5TCU2WDFZ65']),\n TaggedDocument(words=['what', 'daughters', 'orange', 'black', 'white', 'pink', 'think', 'buy', 'fuccia', 'it', 'good', 'way', 'exalt', 'dancer', 'outfit', 'great', 'color', 'comfortable', 'look', 'great', 'easy', 'wear', 'durables', 'little', 'girls', 'love', 'think', 'great', 'buy', 'costumer', 'play'], tags=['A1RLQXYNCMWRWN']),\n TaggedDocument(words=['we', 'buy', 'tutu', 'get', 'high', 'review', 'sturdy', 'seemingly', 'the', 'girl', 'wear', 'regularly', 'include', 'play', 'tutu', 'stand', 'fits', 'yr', 'old', 'yr', 'old', 'clearly', 'plenty', 'room', 'grow', 'only', 'kid', 'pull', 'tutu', 'waste', 'band', 'get', 'twist', 'adult', 'tangle', 'but', 'difficult'], tags=['A8U3FAMSJVHS5']),\n TaggedDocument(words=['thank', 'halo', 'heaven', 'great', 'product', 'little', 'girls', 'my', 'great', 'grand', 'daughters', 'love', 'tutu', 'will', 'buy', 'seller', 'made', 'cute', 'girl', 'thanks', 'great', 'product', 'never', 'buy', 'from', 'dress', 'up', 'dreams', 'buy', 'long', 'buy', 'dress', 'up', 'dreams', 'rec', 'order', 'fl', 'only', 'rec', 'pink', 'purple', 'miss', 'company', 'rip', 'refuses', 'good', 'purchase', 'real', 'creep'], tags=['A3GEOILWLK86XM'])]\n"
     ]
    }
   ],
   "source": [
    "pprint(user_documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[TaggedDocument(words=['this', 'great', 'tutu', 'great', 'price', 'it', 'look', 'cheap', 'glad', 'look', 'amazon', 'affordable', 'tutu', 'poorly'], tags=['0000031887']),\n TaggedDocument(words=['buy', 'yr', 'old', 'daughter', 'dance', 'class', 'wore', 'today', 'time', 'teacher', 'think', 'adorable', 'buy', 'light', 'blue', 'long', 'sleeve', 'leotard', 'happy', 'color', 'match', 'great', 'price', 'good', 'dollar'], tags=['0000031887']),\n TaggedDocument(words=['what', 'daughters', 'orange', 'black', 'white', 'pink', 'think', 'buy', 'fuccia', 'it', 'good', 'way', 'exalt', 'dancer', 'outfit', 'great', 'color', 'comfortable', 'look', 'great', 'easy', 'wear', 'durables', 'little', 'girls', 'love', 'think', 'great', 'buy', 'costumer', 'play'], tags=['0000031887']),\n TaggedDocument(words=['we', 'buy', 'tutu', 'get', 'high', 'review', 'sturdy', 'seemingly', 'the', 'girl', 'wear', 'regularly', 'include', 'play', 'tutu', 'stand', 'fits', 'yr', 'old', 'yr', 'old', 'clearly', 'plenty', 'room', 'grow', 'only', 'kid', 'pull', 'tutu', 'waste', 'band', 'get', 'twist', 'adult', 'tangle', 'but', 'difficult'], tags=['0000031887']),\n TaggedDocument(words=['thank', 'halo', 'heaven', 'great', 'product', 'little', 'girls', 'my', 'great', 'grand', 'daughters', 'love', 'tutu', 'will', 'buy', 'seller', 'made', 'cute', 'girl', 'thanks', 'great', 'product', 'never', 'buy', 'from', 'dress', 'up', 'dreams', 'buy', 'long', 'buy', 'dress', 'up', 'dreams', 'rec', 'order', 'fl', 'only', 'rec', 'pink', 'purple', 'miss', 'company', 'rip', 'refuses', 'good', 'purchase', 'real', 'creep'], tags=['0000031887'])]\n"
     ]
    }
   ],
   "source": [
    "pprint(asin_documents[:5])"
   ]
  },
  {
   "source": [
    "#### 2.3 Training Doc2Vec model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "VECTOR_SIZE = 150\n",
    "MIN_COUNT = 10\n",
    "NEGATIVE = 5\n",
    "NS_EXPONENT = 0.5\n",
    "SAMPLE = 1e-05\n",
    "DM = 1\n",
    "WORKERS = 8\n",
    "EPOCHS = 10\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(message)s')"
   ]
  },
  {
   "source": [
    "##### 2.3.1 `user` d2v model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-19 23:52:28,442 starting a new internal lifecycle event log for Doc2Vec\n",
      "2021-07-19 23:52:28,443 Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d150,n5,w5,mc10,s1e-05,t8)', 'datetime': '2021-07-19T23:52:28.442206', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'created'}\n",
      "2021-07-19 23:52:28,444 collecting all words and their counts\n",
      "2021-07-19 23:52:28,446 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2021-07-19 23:52:28,516 PROGRESS: at example #10000, processed 282592 words (4060309/s), 12776 word types, 8139 tags\n",
      "2021-07-19 23:52:28,577 PROGRESS: at example #20000, processed 569321 words (4795773/s), 18063 word types, 14160 tags\n",
      "2021-07-19 23:52:28,640 PROGRESS: at example #30000, processed 862630 words (4649256/s), 22378 word types, 18865 tags\n",
      "2021-07-19 23:52:28,703 PROGRESS: at example #40000, processed 1152769 words (4673148/s), 26044 word types, 22575 tags\n",
      "2021-07-19 23:52:28,770 PROGRESS: at example #50000, processed 1448737 words (4503142/s), 29204 word types, 25563 tags\n",
      "2021-07-19 23:52:28,833 PROGRESS: at example #60000, processed 1740171 words (4644500/s), 32046 word types, 28071 tags\n",
      "2021-07-19 23:52:28,894 PROGRESS: at example #70000, processed 2018020 words (4639200/s), 34663 word types, 30106 tags\n",
      "2021-07-19 23:52:28,956 PROGRESS: at example #80000, processed 2302381 words (4600244/s), 36946 word types, 31741 tags\n",
      "2021-07-19 23:52:29,018 PROGRESS: at example #90000, processed 2577669 words (4481943/s), 39222 word types, 33146 tags\n",
      "2021-07-19 23:52:29,078 PROGRESS: at example #100000, processed 2850797 words (4609086/s), 41219 word types, 34304 tags\n",
      "2021-07-19 23:52:29,138 PROGRESS: at example #110000, processed 3129821 words (4737727/s), 43179 word types, 35172 tags\n",
      "2021-07-19 23:52:29,199 PROGRESS: at example #120000, processed 3400889 words (4507899/s), 45041 word types, 35956 tags\n",
      "2021-07-19 23:52:29,260 PROGRESS: at example #130000, processed 3682686 words (4673916/s), 46769 word types, 36572 tags\n",
      "2021-07-19 23:52:29,317 PROGRESS: at example #140000, processed 3945660 words (4628288/s), 48394 word types, 37196 tags\n",
      "2021-07-19 23:52:29,381 PROGRESS: at example #150000, processed 4211510 words (4231273/s), 49917 word types, 37793 tags\n",
      "2021-07-19 23:52:29,442 PROGRESS: at example #160000, processed 4468952 words (4289710/s), 51279 word types, 38210 tags\n",
      "2021-07-19 23:52:29,500 PROGRESS: at example #170000, processed 4713176 words (4282674/s), 52628 word types, 38695 tags\n",
      "2021-07-19 23:52:29,557 PROGRESS: at example #180000, processed 4967822 words (4475158/s), 54009 word types, 38917 tags\n",
      "2021-07-19 23:52:29,611 PROGRESS: at example #190000, processed 5200433 words (4366827/s), 55181 word types, 39104 tags\n",
      "2021-07-19 23:52:29,669 PROGRESS: at example #200000, processed 5433152 words (4095757/s), 56375 word types, 39217 tags\n",
      "2021-07-19 23:52:29,731 PROGRESS: at example #210000, processed 5680403 words (4031907/s), 57576 word types, 39301 tags\n",
      "2021-07-19 23:52:29,788 PROGRESS: at example #220000, processed 5913976 words (4204708/s), 58691 word types, 39372 tags\n",
      "2021-07-19 23:52:29,847 PROGRESS: at example #230000, processed 6157011 words (4104189/s), 59829 word types, 39387 tags\n",
      "2021-07-19 23:52:30,055 collected 60048 word types and 39387 unique tags from a corpus of 231491 examples and 6199109 words\n",
      "2021-07-19 23:52:30,055 Creating a fresh vocabulary\n",
      "2021-07-19 23:52:30,113 Doc2Vec lifecycle event {'msg': 'effective_min_count=10 retains 11761 unique words (19.58599786837197%% of original 60048, drops 48287)', 'datetime': '2021-07-19T23:52:30.113401', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-07-19 23:52:30,114 Doc2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 6102692 word corpus (98.44466357987898%% of original 6199109, drops 96417)', 'datetime': '2021-07-19T23:52:30.114066', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-07-19 23:52:30,178 deleting the raw counts dictionary of 60048 items\n",
      "2021-07-19 23:52:30,180 sample=1e-05 downsamples 2830 most-common words\n",
      "2021-07-19 23:52:30,181 Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1261368.0531210527 word corpus (20.7%% of prior 6102692)', 'datetime': '2021-07-19T23:52:30.181125', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-07-19 23:52:30,294 estimated required memory for 11761 words and 150 dimensions: 51503300 bytes\n",
      "2021-07-19 23:52:30,295 resetting layer weights\n",
      "2021-07-19 23:52:30,333 Doc2Vec lifecycle event {'msg': 'training model with 8 workers on 11761 vocabulary and 150 features, using sg=0 hs=0 sample=1e-05 negative=5 window=5', 'datetime': '2021-07-19T23:52:30.333268', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'train'}\n",
      "2021-07-19 23:52:31,396 EPOCH 1 - PROGRESS: at 6.06% examples, 91155 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:32,417 EPOCH 1 - PROGRESS: at 12.21% examples, 98150 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:33,433 EPOCH 1 - PROGRESS: at 18.60% examples, 100491 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:34,443 EPOCH 1 - PROGRESS: at 24.88% examples, 101715 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:35,446 EPOCH 1 - PROGRESS: at 31.22% examples, 101443 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:36,452 EPOCH 1 - PROGRESS: at 37.52% examples, 101203 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:52:37,458 EPOCH 1 - PROGRESS: at 43.56% examples, 99660 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:38,553 EPOCH 1 - PROGRESS: at 49.87% examples, 98415 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:39,575 EPOCH 1 - PROGRESS: at 56.65% examples, 98760 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:40,599 EPOCH 1 - PROGRESS: at 63.56% examples, 98717 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:52:41,635 EPOCH 1 - PROGRESS: at 70.48% examples, 98132 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:52:42,673 EPOCH 1 - PROGRESS: at 76.21% examples, 96059 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:43,688 EPOCH 1 - PROGRESS: at 81.61% examples, 93898 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:44,694 EPOCH 1 - PROGRESS: at 87.60% examples, 92661 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:45,721 EPOCH 1 - PROGRESS: at 93.79% examples, 91747 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:46,348 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:52:46,611 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:52:46,611 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:52:46,619 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:52:46,620 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:52:46,626 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:52:46,626 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:52:46,636 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:52:46,636 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:52:46,639 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:52:46,640 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:52:46,644 worker exiting, processed 76 jobs\n",
      "2021-07-19 23:52:46,653 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:52:46,667 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:52:46,669 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:52:46,678 worker exiting, processed 76 jobs\n",
      "2021-07-19 23:52:46,678 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:52:46,680 EPOCH - 1 : training on 6199109 raw words (1492729 effective words) took 16.3s, 91377 effective words/s\n",
      "2021-07-19 23:52:47,720 EPOCH 2 - PROGRESS: at 5.19% examples, 81342 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:52:48,747 EPOCH 2 - PROGRESS: at 11.91% examples, 96349 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:49,747 EPOCH 2 - PROGRESS: at 17.59% examples, 95660 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:50,760 EPOCH 2 - PROGRESS: at 23.02% examples, 95257 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:51,781 EPOCH 2 - PROGRESS: at 29.20% examples, 95405 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:52,816 EPOCH 2 - PROGRESS: at 35.31% examples, 95286 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:52:53,827 EPOCH 2 - PROGRESS: at 41.91% examples, 95897 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:54,946 EPOCH 2 - PROGRESS: at 48.42% examples, 95189 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:55,988 EPOCH 2 - PROGRESS: at 54.64% examples, 94709 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:57,007 EPOCH 2 - PROGRESS: at 60.82% examples, 94150 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:52:58,035 EPOCH 2 - PROGRESS: at 67.70% examples, 94349 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:52:59,084 EPOCH 2 - PROGRESS: at 73.64% examples, 92736 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:00,117 EPOCH 2 - PROGRESS: at 80.20% examples, 91924 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:01,130 EPOCH 2 - PROGRESS: at 86.41% examples, 90975 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:02,200 EPOCH 2 - PROGRESS: at 92.91% examples, 90195 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:02,850 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:53:03,119 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:03,119 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:53:03,132 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:03,132 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:53:03,157 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:03,157 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:53:03,165 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:03,165 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:53:03,174 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:03,174 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:53:03,182 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:03,186 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:53:03,200 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:53:03,204 EPOCH 2 - PROGRESS: at 99.83% examples, 90224 words/s, in_qsize 1, out_qsize 1\n",
      "2021-07-19 23:53:03,213 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:03,214 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:53:03,215 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:53:03,216 EPOCH - 2 : training on 6199109 raw words (1492508 effective words) took 16.5s, 90302 effective words/s\n",
      "2021-07-19 23:53:04,242 EPOCH 3 - PROGRESS: at 5.90% examples, 93247 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:05,244 EPOCH 3 - PROGRESS: at 12.05% examples, 100264 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:06,252 EPOCH 3 - PROGRESS: at 18.75% examples, 103779 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:07,292 EPOCH 3 - PROGRESS: at 25.36% examples, 104566 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:08,319 EPOCH 3 - PROGRESS: at 31.62% examples, 103387 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:09,347 EPOCH 3 - PROGRESS: at 38.65% examples, 103875 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:10,360 EPOCH 3 - PROGRESS: at 44.83% examples, 102198 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:11,365 EPOCH 3 - PROGRESS: at 51.43% examples, 102253 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:12,415 EPOCH 3 - PROGRESS: at 58.10% examples, 101683 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:13,417 EPOCH 3 - PROGRESS: at 64.88% examples, 101258 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:14,431 EPOCH 3 - PROGRESS: at 70.87% examples, 99346 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:15,443 EPOCH 3 - PROGRESS: at 77.39% examples, 98318 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:16,446 EPOCH 3 - PROGRESS: at 83.81% examples, 96969 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:17,557 EPOCH 3 - PROGRESS: at 90.05% examples, 95125 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:18,560 EPOCH 3 - PROGRESS: at 95.99% examples, 93948 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:18,783 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:53:19,069 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:19,070 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:53:19,074 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:19,075 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:53:19,086 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:19,087 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:53:19,098 worker exiting, processed 76 jobs\n",
      "2021-07-19 23:53:19,099 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:53:19,108 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:19,110 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:53:19,120 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:19,126 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:53:19,143 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:19,144 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:53:19,174 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:53:19,186 EPOCH - 3 : training on 6199109 raw words (1492554 effective words) took 15.9s, 93597 effective words/s\n",
      "2021-07-19 23:53:19,169 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:53:20,247 EPOCH 4 - PROGRESS: at 2.98% examples, 48594 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:21,274 EPOCH 4 - PROGRESS: at 8.60% examples, 68773 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:22,306 EPOCH 4 - PROGRESS: at 14.54% examples, 78143 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:23,321 EPOCH 4 - PROGRESS: at 20.47% examples, 83667 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:24,339 EPOCH 4 - PROGRESS: at 25.51% examples, 83276 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:25,404 EPOCH 4 - PROGRESS: at 31.62% examples, 84962 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:26,455 EPOCH 4 - PROGRESS: at 37.84% examples, 86110 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:27,489 EPOCH 4 - PROGRESS: at 43.20% examples, 85150 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:28,598 EPOCH 4 - PROGRESS: at 48.42% examples, 83863 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:29,676 EPOCH 4 - PROGRESS: at 53.43% examples, 82505 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:30,689 EPOCH 4 - PROGRESS: at 59.19% examples, 82631 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:31,704 EPOCH 4 - PROGRESS: at 65.05% examples, 82720 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:32,718 EPOCH 4 - PROGRESS: at 70.87% examples, 82395 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:33,730 EPOCH 4 - PROGRESS: at 77.56% examples, 82862 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:34,746 EPOCH 4 - PROGRESS: at 83.99% examples, 82660 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:35,873 EPOCH 4 - PROGRESS: at 90.05% examples, 81807 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:36,896 EPOCH 4 - PROGRESS: at 95.99% examples, 81440 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:37,095 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:53:37,368 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:37,369 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:53:37,386 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:37,387 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:53:37,402 worker exiting, processed 76 jobs\n",
      "2021-07-19 23:53:37,403 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:53:37,426 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:37,427 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:53:37,433 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:37,434 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:53:37,435 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:37,439 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:53:37,451 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:53:37,451 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:53:37,458 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:53:37,458 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:53:37,459 EPOCH - 4 : training on 6199109 raw words (1492358 effective words) took 18.2s, 81836 effective words/s\n",
      "2021-07-19 23:53:38,469 EPOCH 5 - PROGRESS: at 5.19% examples, 83997 words/s, in_qsize 14, out_qsize 1\n",
      "2021-07-19 23:53:39,490 EPOCH 5 - PROGRESS: at 11.91% examples, 98203 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:40,498 EPOCH 5 - PROGRESS: at 18.30% examples, 100973 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:41,509 EPOCH 5 - PROGRESS: at 24.58% examples, 101999 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:42,554 EPOCH 5 - PROGRESS: at 30.47% examples, 99470 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:43,594 EPOCH 5 - PROGRESS: at 35.31% examples, 95426 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:44,630 EPOCH 5 - PROGRESS: at 41.60% examples, 95003 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:45,645 EPOCH 5 - PROGRESS: at 47.21% examples, 93901 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:46,647 EPOCH 5 - PROGRESS: at 53.73% examples, 94608 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:47,659 EPOCH 5 - PROGRESS: at 59.55% examples, 93562 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:48,706 EPOCH 5 - PROGRESS: at 65.05% examples, 92000 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:49,718 EPOCH 5 - PROGRESS: at 70.68% examples, 90672 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:50,805 EPOCH 5 - PROGRESS: at 76.07% examples, 88653 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:51,814 EPOCH 5 - PROGRESS: at 81.79% examples, 87531 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:52,832 EPOCH 5 - PROGRESS: at 87.77% examples, 86727 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:53,833 EPOCH 5 - PROGRESS: at 94.16% examples, 86514 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:54,421 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:53:54,706 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:54,706 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:53:54,718 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:53:54,719 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:53:54,723 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:54,724 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:53:54,726 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:54,726 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:53:54,727 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:54,729 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:53:54,741 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:54,741 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:53:54,757 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:53:54,758 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:53:54,761 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:53:54,762 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:53:54,766 EPOCH - 5 : training on 6199109 raw words (1493004 effective words) took 17.3s, 86316 effective words/s\n",
      "2021-07-19 23:53:55,776 EPOCH 6 - PROGRESS: at 4.92% examples, 79052 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:56,799 EPOCH 6 - PROGRESS: at 10.68% examples, 88448 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:53:57,804 EPOCH 6 - PROGRESS: at 16.16% examples, 88492 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:58,810 EPOCH 6 - PROGRESS: at 20.93% examples, 86869 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:53:59,877 EPOCH 6 - PROGRESS: at 26.53% examples, 87024 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:00,886 EPOCH 6 - PROGRESS: at 31.62% examples, 85981 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:01,934 EPOCH 6 - PROGRESS: at 37.52% examples, 86352 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:02,957 EPOCH 6 - PROGRESS: at 42.88% examples, 85474 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:04,049 EPOCH 6 - PROGRESS: at 48.42% examples, 84800 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:05,087 EPOCH 6 - PROGRESS: at 54.33% examples, 84991 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:06,145 EPOCH 6 - PROGRESS: at 60.03% examples, 84385 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:07,166 EPOCH 6 - PROGRESS: at 65.70% examples, 84108 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:08,177 EPOCH 6 - PROGRESS: at 70.87% examples, 83015 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:09,178 EPOCH 6 - PROGRESS: at 77.05% examples, 83042 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:10,179 EPOCH 6 - PROGRESS: at 82.54% examples, 82131 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:11,224 EPOCH 6 - PROGRESS: at 88.67% examples, 81718 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:12,229 EPOCH 6 - PROGRESS: at 94.47% examples, 81387 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:12,671 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:54:12,944 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:12,945 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:54:12,967 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:54:12,967 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:54:12,974 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:12,974 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:54:12,995 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:12,995 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:54:13,007 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:13,008 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:54:13,014 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:13,017 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:54:13,040 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:54:13,044 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:54:13,056 worker exiting, processed 76 jobs\n",
      "2021-07-19 23:54:13,058 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:54:13,062 EPOCH - 6 : training on 6199109 raw words (1493284 effective words) took 18.3s, 81660 effective words/s\n",
      "2021-07-19 23:54:14,084 EPOCH 7 - PROGRESS: at 4.58% examples, 73813 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:15,089 EPOCH 7 - PROGRESS: at 10.10% examples, 83817 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:16,130 EPOCH 7 - PROGRESS: at 15.70% examples, 85403 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:17,134 EPOCH 7 - PROGRESS: at 21.40% examples, 88068 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:18,154 EPOCH 7 - PROGRESS: at 26.69% examples, 87852 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:19,154 EPOCH 7 - PROGRESS: at 32.69% examples, 89261 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:20,166 EPOCH 7 - PROGRESS: at 37.99% examples, 88234 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:21,170 EPOCH 7 - PROGRESS: at 43.73% examples, 87902 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:22,246 EPOCH 7 - PROGRESS: at 49.71% examples, 87830 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:23,264 EPOCH 7 - PROGRESS: at 55.32% examples, 87371 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:24,336 EPOCH 7 - PROGRESS: at 60.77% examples, 86303 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:25,417 EPOCH 7 - PROGRESS: at 66.68% examples, 85637 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:26,420 EPOCH 7 - PROGRESS: at 72.13% examples, 84622 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:27,430 EPOCH 7 - PROGRESS: at 77.22% examples, 83468 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:28,488 EPOCH 7 - PROGRESS: at 82.16% examples, 81771 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:29,514 EPOCH 7 - PROGRESS: at 87.57% examples, 80909 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:30,517 EPOCH 7 - PROGRESS: at 93.24% examples, 80490 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:31,265 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:54:31,551 EPOCH 7 - PROGRESS: at 98.72% examples, 79816 words/s, in_qsize 8, out_qsize 0\n",
      "2021-07-19 23:54:31,576 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:54:31,577 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:54:31,583 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:31,583 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:54:31,601 worker exiting, processed 80 jobs\n",
      "2021-07-19 23:54:31,601 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:54:31,609 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:31,609 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:54:31,620 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:54:31,621 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:54:31,624 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:54:31,627 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:54:31,644 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:31,644 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:54:31,649 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:54:31,649 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:54:31,652 EPOCH - 7 : training on 6199109 raw words (1492413 effective words) took 18.6s, 80365 effective words/s\n",
      "2021-07-19 23:54:32,754 EPOCH 8 - PROGRESS: at 5.07% examples, 74468 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:33,770 EPOCH 8 - PROGRESS: at 10.68% examples, 84847 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:34,785 EPOCH 8 - PROGRESS: at 16.16% examples, 85722 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:35,794 EPOCH 8 - PROGRESS: at 21.80% examples, 88282 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:36,848 EPOCH 8 - PROGRESS: at 27.78% examples, 89326 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:37,866 EPOCH 8 - PROGRESS: at 33.29% examples, 89178 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:38,895 EPOCH 8 - PROGRESS: at 38.94% examples, 88426 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:39,896 EPOCH 8 - PROGRESS: at 45.00% examples, 88738 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:40,902 EPOCH 8 - PROGRESS: at 51.11% examples, 89464 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:41,928 EPOCH 8 - PROGRESS: at 57.49% examples, 89996 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:42,957 EPOCH 8 - PROGRESS: at 63.56% examples, 89634 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:43,987 EPOCH 8 - PROGRESS: at 69.45% examples, 88784 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:45,027 EPOCH 8 - PROGRESS: at 76.07% examples, 88479 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:46,051 EPOCH 8 - PROGRESS: at 82.54% examples, 87936 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:47,091 EPOCH 8 - PROGRESS: at 89.00% examples, 87424 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:48,100 EPOCH 8 - PROGRESS: at 95.70% examples, 87313 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:48,406 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:54:48,659 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:48,660 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:54:48,666 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:48,667 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:54:48,688 worker exiting, processed 76 jobs\n",
      "2021-07-19 23:54:48,702 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:54:48,704 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:54:48,708 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:54:48,714 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:54:48,714 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:54:48,725 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:54:48,725 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:54:48,735 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:54:48,740 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:54:48,742 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:54:48,743 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:54:48,744 EPOCH - 8 : training on 6199109 raw words (1493519 effective words) took 17.1s, 87428 effective words/s\n",
      "2021-07-19 23:54:49,756 EPOCH 9 - PROGRESS: at 5.55% examples, 88691 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:50,777 EPOCH 9 - PROGRESS: at 11.91% examples, 98059 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:51,790 EPOCH 9 - PROGRESS: at 18.22% examples, 99943 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:52,806 EPOCH 9 - PROGRESS: at 24.42% examples, 101170 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:53,807 EPOCH 9 - PROGRESS: at 31.09% examples, 102070 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:54,827 EPOCH 9 - PROGRESS: at 37.52% examples, 101894 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:55,842 EPOCH 9 - PROGRESS: at 44.05% examples, 101142 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:56,843 EPOCH 9 - PROGRESS: at 50.65% examples, 101420 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:54:57,878 EPOCH 9 - PROGRESS: at 56.48% examples, 99701 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:58,880 EPOCH 9 - PROGRESS: at 62.82% examples, 98861 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:54:59,995 EPOCH 9 - PROGRESS: at 69.45% examples, 97388 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:01,017 EPOCH 9 - PROGRESS: at 76.07% examples, 96467 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:02,041 EPOCH 9 - PROGRESS: at 82.54% examples, 95243 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:03,076 EPOCH 9 - PROGRESS: at 89.00% examples, 94175 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:04,094 EPOCH 9 - PROGRESS: at 95.70% examples, 93549 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:04,397 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:55:04,660 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:55:04,661 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:55:04,667 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:55:04,667 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:55:04,687 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:55:04,687 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:55:04,691 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:55:04,691 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:55:04,695 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:55:04,695 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:55:04,708 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:55:04,709 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:55:04,723 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:55:04,724 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:55:04,728 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:55:04,729 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:55:04,731 EPOCH - 9 : training on 6199109 raw words (1493508 effective words) took 16.0s, 93473 effective words/s\n",
      "2021-07-19 23:55:05,757 EPOCH 10 - PROGRESS: at 5.19% examples, 82691 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:06,761 EPOCH 10 - PROGRESS: at 11.76% examples, 96991 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:07,763 EPOCH 10 - PROGRESS: at 17.90% examples, 98671 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:08,832 EPOCH 10 - PROGRESS: at 24.13% examples, 98910 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:55:09,839 EPOCH 10 - PROGRESS: at 30.47% examples, 99145 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:10,867 EPOCH 10 - PROGRESS: at 36.57% examples, 98536 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-19 23:55:11,878 EPOCH 10 - PROGRESS: at 43.04% examples, 98300 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:12,973 EPOCH 10 - PROGRESS: at 49.71% examples, 97839 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:13,983 EPOCH 10 - PROGRESS: at 56.02% examples, 97608 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:15,000 EPOCH 10 - PROGRESS: at 62.49% examples, 97041 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:16,015 EPOCH 10 - PROGRESS: at 68.91% examples, 96388 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:17,038 EPOCH 10 - PROGRESS: at 75.04% examples, 94964 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:18,061 EPOCH 10 - PROGRESS: at 81.61% examples, 94046 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:19,071 EPOCH 10 - PROGRESS: at 88.31% examples, 93421 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:20,090 EPOCH 10 - PROGRESS: at 94.32% examples, 92337 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-19 23:55:20,509 job loop exiting, total 622 jobs\n",
      "2021-07-19 23:55:20,764 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:55:20,765 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-19 23:55:20,786 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:55:20,787 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-19 23:55:20,794 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:55:20,794 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-19 23:55:20,799 worker exiting, processed 79 jobs\n",
      "2021-07-19 23:55:20,799 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-19 23:55:20,804 worker exiting, processed 76 jobs\n",
      "2021-07-19 23:55:20,805 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-19 23:55:20,812 worker exiting, processed 80 jobs\n",
      "2021-07-19 23:55:20,812 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-19 23:55:20,830 worker exiting, processed 78 jobs\n",
      "2021-07-19 23:55:20,830 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-19 23:55:20,834 worker exiting, processed 77 jobs\n",
      "2021-07-19 23:55:20,835 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-19 23:55:20,838 EPOCH - 10 : training on 6199109 raw words (1492286 effective words) took 16.1s, 92707 effective words/s\n",
      "2021-07-19 23:55:20,838 Doc2Vec lifecycle event {'msg': 'training on 61991090 raw words (14928163 effective words) took 170.5s, 87554 effective words/s', 'datetime': '2021-07-19T23:55:20.838694', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "user_d2v = Doc2Vec(\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    negative=NEGATIVE,\n",
    "    sample=SAMPLE,\n",
    "    ns_exponent=NS_EXPONENT,\n",
    "    dm=DM,\n",
    "    workers=WORKERS,\n",
    ")\n",
    "\n",
    "# building vocab\n",
    "user_d2v.build_vocab(user_documents)\n",
    "\n",
    "# training model\n",
    "user_d2v.train(user_documents, total_examples=user_d2v.corpus_count, epochs=EPOCHS)"
   ]
  },
  {
   "source": [
    "##### 2.3.2 `asin` d2v model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-20 00:05:07,171 starting a new internal lifecycle event log for Doc2Vec\n",
      "2021-07-20 00:05:07,172 Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d150,n5,w5,mc10,s1e-05,t8)', 'datetime': '2021-07-20T00:05:07.171231', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'created'}\n",
      "2021-07-20 00:05:07,173 collecting all words and their counts\n",
      "2021-07-20 00:05:07,176 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2021-07-20 00:05:07,237 PROGRESS: at example #10000, processed 282592 words (4685298/s), 12776 word types, 677 tags\n",
      "2021-07-20 00:05:07,293 PROGRESS: at example #20000, processed 569321 words (5214871/s), 18063 word types, 1392 tags\n",
      "2021-07-20 00:05:07,351 PROGRESS: at example #30000, processed 862630 words (5068506/s), 22378 word types, 2208 tags\n",
      "2021-07-20 00:05:07,411 PROGRESS: at example #40000, processed 1152769 words (4908405/s), 26044 word types, 3120 tags\n",
      "2021-07-20 00:05:07,474 PROGRESS: at example #50000, processed 1448737 words (4944518/s), 29204 word types, 3990 tags\n",
      "2021-07-20 00:05:07,537 PROGRESS: at example #60000, processed 1740171 words (4682380/s), 32046 word types, 4944 tags\n",
      "2021-07-20 00:05:07,595 PROGRESS: at example #70000, processed 2018020 words (4826463/s), 34663 word types, 5965 tags\n",
      "2021-07-20 00:05:07,653 PROGRESS: at example #80000, processed 2302381 words (5014984/s), 36946 word types, 6933 tags\n",
      "2021-07-20 00:05:07,709 PROGRESS: at example #90000, processed 2577669 words (4920792/s), 39222 word types, 7975 tags\n",
      "2021-07-20 00:05:07,770 PROGRESS: at example #100000, processed 2850797 words (4529924/s), 41219 word types, 9034 tags\n",
      "2021-07-20 00:05:07,832 PROGRESS: at example #110000, processed 3129821 words (4540308/s), 43179 word types, 10012 tags\n",
      "2021-07-20 00:05:07,889 PROGRESS: at example #120000, processed 3400889 words (4813691/s), 45041 word types, 11054 tags\n",
      "2021-07-20 00:05:07,950 PROGRESS: at example #130000, processed 3682686 words (4725562/s), 46769 word types, 12091 tags\n",
      "2021-07-20 00:05:08,003 PROGRESS: at example #140000, processed 3945660 words (5025072/s), 48394 word types, 13090 tags\n",
      "2021-07-20 00:05:08,061 PROGRESS: at example #150000, processed 4211510 words (4592912/s), 49917 word types, 14105 tags\n",
      "2021-07-20 00:05:08,113 PROGRESS: at example #160000, processed 4468952 words (5098494/s), 51279 word types, 15145 tags\n",
      "2021-07-20 00:05:08,167 PROGRESS: at example #170000, processed 4713176 words (4572736/s), 52628 word types, 16104 tags\n",
      "2021-07-20 00:05:08,218 PROGRESS: at example #180000, processed 4967822 words (5085331/s), 54009 word types, 17224 tags\n",
      "2021-07-20 00:05:08,270 PROGRESS: at example #190000, processed 5200433 words (4470607/s), 55181 word types, 18281 tags\n",
      "2021-07-20 00:05:08,321 PROGRESS: at example #200000, processed 5433152 words (4687063/s), 56375 word types, 19375 tags\n",
      "2021-07-20 00:05:08,379 PROGRESS: at example #210000, processed 5680403 words (4267841/s), 57576 word types, 20551 tags\n",
      "2021-07-20 00:05:08,429 PROGRESS: at example #220000, processed 5913976 words (4736911/s), 58691 word types, 21611 tags\n",
      "2021-07-20 00:05:08,484 PROGRESS: at example #230000, processed 6157011 words (4493673/s), 59829 word types, 22816 tags\n",
      "2021-07-20 00:05:08,604 collected 60048 word types and 23031 unique tags from a corpus of 231491 examples and 6199109 words\n",
      "2021-07-20 00:05:08,605 Creating a fresh vocabulary\n",
      "2021-07-20 00:05:08,663 Doc2Vec lifecycle event {'msg': 'effective_min_count=10 retains 11761 unique words (19.58599786837197%% of original 60048, drops 48287)', 'datetime': '2021-07-20T00:05:08.663127', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-07-20 00:05:08,663 Doc2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 6102692 word corpus (98.44466357987898%% of original 6199109, drops 96417)', 'datetime': '2021-07-20T00:05:08.663859', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-07-20 00:05:08,730 deleting the raw counts dictionary of 60048 items\n",
      "2021-07-20 00:05:08,731 sample=1e-05 downsamples 2830 most-common words\n",
      "2021-07-20 00:05:08,732 Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1261368.0531210527 word corpus (20.7%% of prior 6102692)', 'datetime': '2021-07-20T00:05:08.732540', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2021-07-20 00:05:08,848 estimated required memory for 11761 words and 150 dimensions: 38418500 bytes\n",
      "2021-07-20 00:05:08,848 resetting layer weights\n",
      "2021-07-20 00:05:08,874 Doc2Vec lifecycle event {'msg': 'training model with 8 workers on 11761 vocabulary and 150 features, using sg=0 hs=0 sample=1e-05 negative=5 window=5', 'datetime': '2021-07-20T00:05:08.874490', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'train'}\n",
      "2021-07-20 00:05:09,932 EPOCH 1 - PROGRESS: at 6.06% examples, 91487 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:10,951 EPOCH 1 - PROGRESS: at 12.69% examples, 101795 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:11,963 EPOCH 1 - PROGRESS: at 19.32% examples, 104755 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:13,006 EPOCH 1 - PROGRESS: at 26.53% examples, 107528 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:14,019 EPOCH 1 - PROGRESS: at 33.61% examples, 108535 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:15,036 EPOCH 1 - PROGRESS: at 40.36% examples, 107412 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:16,044 EPOCH 1 - PROGRESS: at 47.35% examples, 107393 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:17,082 EPOCH 1 - PROGRESS: at 54.64% examples, 107380 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:05:18,100 EPOCH 1 - PROGRESS: at 61.63% examples, 106660 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:19,108 EPOCH 1 - PROGRESS: at 68.91% examples, 106218 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:20,154 EPOCH 1 - PROGRESS: at 76.07% examples, 104821 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:21,192 EPOCH 1 - PROGRESS: at 83.45% examples, 103650 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:22,222 EPOCH 1 - PROGRESS: at 90.54% examples, 102627 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:05:23,187 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:05:23,232 EPOCH 1 - PROGRESS: at 97.62% examples, 101723 words/s, in_qsize 14, out_qsize 0\n",
      "2021-07-20 00:05:23,428 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:23,429 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:05:23,430 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:23,430 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:05:23,442 worker exiting, processed 76 jobs\n",
      "2021-07-20 00:05:23,442 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:05:23,450 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:05:23,450 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:05:23,458 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:05:23,458 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:05:23,467 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:05:23,468 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:05:23,481 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:05:23,481 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:05:23,487 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:23,487 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:05:23,488 EPOCH - 1 : training on 6199109 raw words (1491889 effective words) took 14.6s, 102148 effective words/s\n",
      "2021-07-20 00:05:24,498 EPOCH 2 - PROGRESS: at 6.36% examples, 100601 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:25,543 EPOCH 2 - PROGRESS: at 13.30% examples, 107959 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:26,613 EPOCH 2 - PROGRESS: at 20.63% examples, 110870 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:05:27,658 EPOCH 2 - PROGRESS: at 27.92% examples, 112051 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:28,673 EPOCH 2 - PROGRESS: at 35.14% examples, 112395 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:29,682 EPOCH 2 - PROGRESS: at 41.76% examples, 110395 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:30,687 EPOCH 2 - PROGRESS: at 48.56% examples, 109820 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:31,690 EPOCH 2 - PROGRESS: at 55.85% examples, 109862 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:32,693 EPOCH 2 - PROGRESS: at 62.64% examples, 108555 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:33,736 EPOCH 2 - PROGRESS: at 69.45% examples, 106881 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:34,739 EPOCH 2 - PROGRESS: at 77.05% examples, 106381 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:35,761 EPOCH 2 - PROGRESS: at 83.99% examples, 104680 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:05:36,834 EPOCH 2 - PROGRESS: at 91.48% examples, 103576 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:37,664 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:05:37,900 EPOCH 2 - PROGRESS: at 98.72% examples, 102364 words/s, in_qsize 8, out_qsize 0\n",
      "2021-07-20 00:05:37,909 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:37,909 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:05:37,918 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:37,918 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:05:37,931 worker exiting, processed 76 jobs\n",
      "2021-07-20 00:05:37,932 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:05:37,936 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:37,937 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:05:37,945 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:37,945 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:05:37,951 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:05:37,951 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:05:37,967 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:05:37,968 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:05:37,974 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:37,974 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:05:37,976 EPOCH - 2 : training on 6199109 raw words (1492525 effective words) took 14.5s, 103091 effective words/s\n",
      "2021-07-20 00:05:38,986 EPOCH 3 - PROGRESS: at 6.06% examples, 95853 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:40,009 EPOCH 3 - PROGRESS: at 13.16% examples, 107816 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:41,010 EPOCH 3 - PROGRESS: at 20.21% examples, 111672 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:42,018 EPOCH 3 - PROGRESS: at 26.69% examples, 110634 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:43,029 EPOCH 3 - PROGRESS: at 33.92% examples, 111538 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:44,042 EPOCH 3 - PROGRESS: at 40.82% examples, 110276 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:45,059 EPOCH 3 - PROGRESS: at 47.96% examples, 110149 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:46,063 EPOCH 3 - PROGRESS: at 54.64% examples, 109064 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:05:47,068 EPOCH 3 - PROGRESS: at 61.63% examples, 108307 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:48,070 EPOCH 3 - PROGRESS: at 68.58% examples, 107325 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:49,071 EPOCH 3 - PROGRESS: at 75.88% examples, 106444 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:05:50,079 EPOCH 3 - PROGRESS: at 82.71% examples, 104765 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:51,148 EPOCH 3 - PROGRESS: at 90.05% examples, 103487 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:52,181 EPOCH 3 - PROGRESS: at 97.27% examples, 102546 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:52,181 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:05:52,417 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:05:52,417 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:05:52,428 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:52,429 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:05:52,434 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:05:52,435 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:05:52,438 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:05:52,439 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:05:52,445 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:05:52,446 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:05:52,448 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:52,449 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:05:52,470 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:05:52,470 worker exiting, processed 76 jobs\n",
      "2021-07-20 00:05:52,470 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:05:52,474 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:05:52,475 EPOCH - 3 : training on 6199109 raw words (1492744 effective words) took 14.5s, 103020 effective words/s\n",
      "2021-07-20 00:05:53,508 EPOCH 4 - PROGRESS: at 6.36% examples, 98297 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:54,548 EPOCH 4 - PROGRESS: at 13.30% examples, 106964 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:55,615 EPOCH 4 - PROGRESS: at 20.63% examples, 110300 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:56,616 EPOCH 4 - PROGRESS: at 27.78% examples, 112144 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:57,634 EPOCH 4 - PROGRESS: at 34.85% examples, 112019 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:58,670 EPOCH 4 - PROGRESS: at 41.60% examples, 109959 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:05:59,697 EPOCH 4 - PROGRESS: at 48.42% examples, 109101 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:00,762 EPOCH 4 - PROGRESS: at 56.02% examples, 109035 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:01,764 EPOCH 4 - PROGRESS: at 63.12% examples, 108378 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:02,771 EPOCH 4 - PROGRESS: at 70.28% examples, 107605 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:03,851 EPOCH 4 - PROGRESS: at 77.39% examples, 105646 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:04,867 EPOCH 4 - PROGRESS: at 84.57% examples, 104252 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:05,882 EPOCH 4 - PROGRESS: at 91.84% examples, 103481 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:06,676 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:06:06,899 EPOCH 4 - PROGRESS: at 98.72% examples, 102330 words/s, in_qsize 8, out_qsize 0\n",
      "2021-07-20 00:06:06,916 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:06,917 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:06:06,922 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:06:06,922 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:06:06,939 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:06:06,940 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:06:06,948 worker exiting, processed 76 jobs\n",
      "2021-07-20 00:06:06,949 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:06:06,949 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:06:06,950 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:06:06,959 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:06,959 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:06:06,971 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:06,972 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:06:06,978 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:06,979 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:06:06,979 EPOCH - 4 : training on 6199109 raw words (1493489 effective words) took 14.5s, 103029 effective words/s\n",
      "2021-07-20 00:06:08,014 EPOCH 5 - PROGRESS: at 6.36% examples, 98374 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:09,064 EPOCH 5 - PROGRESS: at 13.30% examples, 106277 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:10,126 EPOCH 5 - PROGRESS: at 20.63% examples, 109892 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:11,163 EPOCH 5 - PROGRESS: at 27.78% examples, 110828 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:12,176 EPOCH 5 - PROGRESS: at 34.85% examples, 111049 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:13,182 EPOCH 5 - PROGRESS: at 41.60% examples, 109662 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:14,207 EPOCH 5 - PROGRESS: at 48.42% examples, 108902 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:15,273 EPOCH 5 - PROGRESS: at 56.02% examples, 108807 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:16,280 EPOCH 5 - PROGRESS: at 63.12% examples, 108063 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:17,282 EPOCH 5 - PROGRESS: at 70.28% examples, 107394 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:18,283 EPOCH 5 - PROGRESS: at 77.22% examples, 106026 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:19,290 EPOCH 5 - PROGRESS: at 83.99% examples, 104260 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:20,365 EPOCH 5 - PROGRESS: at 91.48% examples, 103183 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:21,197 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:06:21,424 EPOCH 5 - PROGRESS: at 98.72% examples, 102085 words/s, in_qsize 8, out_qsize 0\n",
      "2021-07-20 00:06:21,436 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:21,436 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:06:21,445 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:21,445 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:06:21,461 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:06:21,461 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:06:21,467 worker exiting, processed 76 jobs\n",
      "2021-07-20 00:06:21,468 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:06:21,475 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:21,475 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:06:21,478 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:21,479 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:06:21,494 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:21,494 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:06:21,498 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:21,498 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:06:21,500 EPOCH - 5 : training on 6199109 raw words (1491895 effective words) took 14.5s, 102813 effective words/s\n",
      "2021-07-20 00:06:22,524 EPOCH 6 - PROGRESS: at 6.36% examples, 99405 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:23,574 EPOCH 6 - PROGRESS: at 13.30% examples, 106873 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:24,645 EPOCH 6 - PROGRESS: at 20.63% examples, 109949 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:25,649 EPOCH 6 - PROGRESS: at 27.78% examples, 111740 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:26,665 EPOCH 6 - PROGRESS: at 34.85% examples, 111837 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:27,665 EPOCH 6 - PROGRESS: at 41.60% examples, 110365 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:28,705 EPOCH 6 - PROGRESS: at 48.42% examples, 109253 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:29,765 EPOCH 6 - PROGRESS: at 56.02% examples, 109244 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:30,774 EPOCH 6 - PROGRESS: at 63.12% examples, 108474 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:31,833 EPOCH 6 - PROGRESS: at 70.48% examples, 107348 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:32,838 EPOCH 6 - PROGRESS: at 77.39% examples, 105937 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:33,854 EPOCH 6 - PROGRESS: at 84.57% examples, 104512 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:34,860 EPOCH 6 - PROGRESS: at 91.66% examples, 103612 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:35,679 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:06:35,898 EPOCH 6 - PROGRESS: at 98.72% examples, 102463 words/s, in_qsize 8, out_qsize 0\n",
      "2021-07-20 00:06:35,917 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:35,917 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:06:35,921 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:35,921 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:06:35,933 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:35,934 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:06:35,939 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:35,941 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:06:35,955 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:06:35,956 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:06:35,957 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:35,961 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:06:35,974 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:35,974 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:06:35,979 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:35,979 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:06:35,981 EPOCH - 6 : training on 6199109 raw words (1492477 effective words) took 14.5s, 103132 effective words/s\n",
      "2021-07-20 00:06:37,008 EPOCH 7 - PROGRESS: at 6.36% examples, 99613 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:38,069 EPOCH 7 - PROGRESS: at 13.30% examples, 106717 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:39,070 EPOCH 7 - PROGRESS: at 20.47% examples, 111705 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:40,089 EPOCH 7 - PROGRESS: at 27.01% examples, 110288 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:41,141 EPOCH 7 - PROGRESS: at 34.08% examples, 109880 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:42,143 EPOCH 7 - PROGRESS: at 41.30% examples, 109855 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:43,231 EPOCH 7 - PROGRESS: at 48.42% examples, 108770 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:44,293 EPOCH 7 - PROGRESS: at 56.02% examples, 108794 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:45,298 EPOCH 7 - PROGRESS: at 63.12% examples, 108109 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:46,300 EPOCH 7 - PROGRESS: at 70.28% examples, 107395 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:47,321 EPOCH 7 - PROGRESS: at 77.22% examples, 105846 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:48,346 EPOCH 7 - PROGRESS: at 83.99% examples, 103939 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:49,413 EPOCH 7 - PROGRESS: at 91.48% examples, 102944 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:50,241 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:06:50,460 EPOCH 7 - PROGRESS: at 98.72% examples, 101943 words/s, in_qsize 8, out_qsize 0\n",
      "2021-07-20 00:06:50,478 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:06:50,478 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:06:50,484 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:50,484 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:06:50,497 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:50,498 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:06:50,506 worker exiting, processed 76 jobs\n",
      "2021-07-20 00:06:50,507 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:06:50,512 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:06:50,513 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:06:50,517 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:50,518 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:06:50,530 worker exiting, processed 80 jobs\n",
      "2021-07-20 00:06:50,530 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:06:50,536 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:06:50,537 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:06:50,539 EPOCH - 7 : training on 6199109 raw words (1492909 effective words) took 14.5s, 102654 effective words/s\n",
      "2021-07-20 00:06:51,574 EPOCH 8 - PROGRESS: at 6.36% examples, 98227 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:52,615 EPOCH 8 - PROGRESS: at 13.30% examples, 106675 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:53,618 EPOCH 8 - PROGRESS: at 20.47% examples, 111651 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:54,622 EPOCH 8 - PROGRESS: at 27.29% examples, 111906 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:06:55,649 EPOCH 8 - PROGRESS: at 34.08% examples, 110815 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:56,667 EPOCH 8 - PROGRESS: at 41.30% examples, 110405 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:57,747 EPOCH 8 - PROGRESS: at 48.42% examples, 109277 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:58,749 EPOCH 8 - PROGRESS: at 55.68% examples, 109485 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:06:59,783 EPOCH 8 - PROGRESS: at 62.82% examples, 108380 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:00,796 EPOCH 8 - PROGRESS: at 69.62% examples, 107053 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:01,808 EPOCH 8 - PROGRESS: at 77.22% examples, 106475 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:02,819 EPOCH 8 - PROGRESS: at 83.99% examples, 104678 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:03,899 EPOCH 8 - PROGRESS: at 91.48% examples, 103517 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:04,733 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:07:04,956 EPOCH 8 - PROGRESS: at 98.72% examples, 102411 words/s, in_qsize 8, out_qsize 0\n",
      "2021-07-20 00:07:04,968 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:04,968 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:07:04,976 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:04,976 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:07:04,984 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:04,985 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:07:04,989 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:04,990 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:07:04,999 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:04,999 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:07:05,010 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:05,010 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:07:05,023 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:05,024 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:07:05,025 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:07:05,025 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:07:05,027 EPOCH - 8 : training on 6199109 raw words (1493695 effective words) took 14.5s, 103176 effective words/s\n",
      "2021-07-20 00:07:06,062 EPOCH 9 - PROGRESS: at 6.36% examples, 98076 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:07,108 EPOCH 9 - PROGRESS: at 13.30% examples, 106695 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:07:08,112 EPOCH 9 - PROGRESS: at 20.47% examples, 111565 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:09,115 EPOCH 9 - PROGRESS: at 27.29% examples, 111837 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:10,142 EPOCH 9 - PROGRESS: at 34.08% examples, 110750 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:07:11,145 EPOCH 9 - PROGRESS: at 41.30% examples, 110618 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:12,222 EPOCH 9 - PROGRESS: at 48.42% examples, 109544 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:13,224 EPOCH 9 - PROGRESS: at 55.85% examples, 109945 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:07:14,252 EPOCH 9 - PROGRESS: at 62.82% examples, 108607 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:15,259 EPOCH 9 - PROGRESS: at 69.62% examples, 107322 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:16,271 EPOCH 9 - PROGRESS: at 77.22% examples, 106709 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:17,276 EPOCH 9 - PROGRESS: at 83.99% examples, 104940 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:18,283 EPOCH 9 - PROGRESS: at 91.30% examples, 104160 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:19,195 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:07:19,290 EPOCH 9 - PROGRESS: at 98.37% examples, 103197 words/s, in_qsize 10, out_qsize 0\n",
      "2021-07-20 00:07:19,434 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:19,434 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:07:19,444 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:19,445 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:07:19,453 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:19,453 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:07:19,458 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:07:19,459 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:07:19,468 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:19,468 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:07:19,470 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:19,471 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:07:19,490 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:19,490 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:07:19,494 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:19,496 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:07:19,499 EPOCH - 9 : training on 6199109 raw words (1494171 effective words) took 14.5s, 103302 effective words/s\n",
      "2021-07-20 00:07:20,522 EPOCH 10 - PROGRESS: at 6.36% examples, 99590 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:21,579 EPOCH 10 - PROGRESS: at 13.30% examples, 106777 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:22,656 EPOCH 10 - PROGRESS: at 20.63% examples, 109633 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:23,668 EPOCH 10 - PROGRESS: at 27.78% examples, 111391 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:24,675 EPOCH 10 - PROGRESS: at 34.85% examples, 111690 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:25,713 EPOCH 10 - PROGRESS: at 41.76% examples, 110055 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:07:26,718 EPOCH 10 - PROGRESS: at 48.56% examples, 109481 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:07:27,765 EPOCH 10 - PROGRESS: at 56.02% examples, 109306 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:07:28,786 EPOCH 10 - PROGRESS: at 63.12% examples, 108342 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:29,789 EPOCH 10 - PROGRESS: at 70.10% examples, 107410 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:30,850 EPOCH 10 - PROGRESS: at 77.39% examples, 105883 words/s, in_qsize 16, out_qsize 0\n",
      "2021-07-20 00:07:31,870 EPOCH 10 - PROGRESS: at 84.57% examples, 104431 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:32,880 EPOCH 10 - PROGRESS: at 91.66% examples, 103505 words/s, in_qsize 15, out_qsize 0\n",
      "2021-07-20 00:07:33,694 job loop exiting, total 622 jobs\n",
      "2021-07-20 00:07:33,920 EPOCH 10 - PROGRESS: at 98.72% examples, 102375 words/s, in_qsize 8, out_qsize 0\n",
      "2021-07-20 00:07:33,933 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:07:33,934 worker thread finished; awaiting finish of 7 more threads\n",
      "2021-07-20 00:07:33,943 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:33,944 worker thread finished; awaiting finish of 6 more threads\n",
      "2021-07-20 00:07:33,949 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:07:33,949 worker thread finished; awaiting finish of 5 more threads\n",
      "2021-07-20 00:07:33,957 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:33,957 worker thread finished; awaiting finish of 4 more threads\n",
      "2021-07-20 00:07:33,965 worker exiting, processed 78 jobs\n",
      "2021-07-20 00:07:33,965 worker thread finished; awaiting finish of 3 more threads\n",
      "2021-07-20 00:07:33,971 worker exiting, processed 76 jobs\n",
      "2021-07-20 00:07:33,971 worker thread finished; awaiting finish of 2 more threads\n",
      "2021-07-20 00:07:33,988 worker exiting, processed 79 jobs\n",
      "2021-07-20 00:07:33,988 worker thread finished; awaiting finish of 1 more threads\n",
      "2021-07-20 00:07:33,992 worker exiting, processed 77 jobs\n",
      "2021-07-20 00:07:33,992 worker thread finished; awaiting finish of 0 more threads\n",
      "2021-07-20 00:07:33,994 EPOCH - 10 : training on 6199109 raw words (1493490 effective words) took 14.5s, 103114 effective words/s\n",
      "2021-07-20 00:07:33,996 Doc2Vec lifecycle event {'msg': 'training on 61991090 raw words (14929284 effective words) took 145.1s, 102878 effective words/s', 'datetime': '2021-07-20T00:07:33.996603', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "asin_d2v = Doc2Vec(\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    negative=NEGATIVE,\n",
    "    sample=SAMPLE,\n",
    "    ns_exponent=NS_EXPONENT,\n",
    "    dm=DM,\n",
    "    workers=WORKERS,\n",
    ")\n",
    "\n",
    "# building vocab\n",
    "asin_d2v.build_vocab(asin_documents)\n",
    "\n",
    "# training model\n",
    "asin_d2v.train(asin_documents, total_examples=asin_d2v.corpus_count, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-20 00:31:59,088 Doc2Vec lifecycle event {'fname_or_handle': 'models/d2v/Clothing_Shoes_and_Jewelry_user_d2v.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-07-20T00:31:59.088398', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2021-07-20 00:31:59,089 not storing attribute cum_table\n",
      "2021-07-20 00:31:59,090 {'uri': 'models/d2v/Clothing_Shoes_and_Jewelry_user_d2v.model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'ignore_ext': False, 'compression': None, 'transport_params': None}\n",
      "2021-07-20 00:31:59,147 saved models/d2v/Clothing_Shoes_and_Jewelry_user_d2v.model\n",
      "2021-07-20 00:31:59,148 Doc2Vec lifecycle event {'fname_or_handle': 'models/d2v/Clothing_Shoes_and_Jewelry_asin_d2v.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-07-20T00:31:59.148374', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2021-07-20 00:31:59,149 not storing attribute cum_table\n",
      "2021-07-20 00:31:59,149 {'uri': 'models/d2v/Clothing_Shoes_and_Jewelry_asin_d2v.model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'ignore_ext': False, 'compression': None, 'transport_params': None}\n",
      "2021-07-20 00:31:59,189 saved models/d2v/Clothing_Shoes_and_Jewelry_asin_d2v.model\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = Path(\"models/d2v/\")\n",
    "\n",
    "user_d2v.save(f\"{MODEL_PATH}/{CATEGORY}_user_d2v.model\")\n",
    "asin_d2v.save(f\"{MODEL_PATH}/{CATEGORY}_asin_d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-20 00:32:51,835 loading Doc2Vec object from models/d2v/Clothing_Shoes_and_Jewelry_user_d2v.model\n",
      "2021-07-20 00:32:51,840 {'uri': 'models/d2v/Clothing_Shoes_and_Jewelry_user_d2v.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'ignore_ext': False, 'compression': None, 'transport_params': None}\n",
      "2021-07-20 00:32:51,878 loading dv recursively from models/d2v/Clothing_Shoes_and_Jewelry_user_d2v.model.dv.* with mmap=None\n",
      "2021-07-20 00:32:51,879 loading wv recursively from models/d2v/Clothing_Shoes_and_Jewelry_user_d2v.model.wv.* with mmap=None\n",
      "2021-07-20 00:32:51,881 setting ignored attribute cum_table to None\n",
      "2021-07-20 00:32:52,012 Doc2Vec lifecycle event {'fname': 'models/d2v/Clothing_Shoes_and_Jewelry_user_d2v.model', 'datetime': '2021-07-20T00:32:52.012475', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'loaded'}\n",
      "2021-07-20 00:32:52,013 loading Doc2Vec object from models/d2v/Clothing_Shoes_and_Jewelry_asin_d2v.model\n",
      "2021-07-20 00:32:52,013 {'uri': 'models/d2v/Clothing_Shoes_and_Jewelry_asin_d2v.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'ignore_ext': False, 'compression': None, 'transport_params': None}\n",
      "2021-07-20 00:32:52,043 loading dv recursively from models/d2v/Clothing_Shoes_and_Jewelry_asin_d2v.model.dv.* with mmap=None\n",
      "2021-07-20 00:32:52,044 loading wv recursively from models/d2v/Clothing_Shoes_and_Jewelry_asin_d2v.model.wv.* with mmap=None\n",
      "2021-07-20 00:32:52,046 setting ignored attribute cum_table to None\n",
      "2021-07-20 00:32:52,181 Doc2Vec lifecycle event {'fname': 'models/d2v/Clothing_Shoes_and_Jewelry_asin_d2v.model', 'datetime': '2021-07-20T00:32:52.181623', 'gensim': '4.0.1', 'python': '3.9.4 (default, Apr  5 2021, 01:50:46) \\n[Clang 12.0.0 (clang-1200.0.32.29)]', 'platform': 'macOS-11.4-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "MODEL_PATH = Path(\"models/d2v/\")\n",
    "\n",
    "user_d2v = Doc2Vec.load(f\"{MODEL_PATH}/{CATEGORY}_user_d2v.model\")\n",
    "asin_d2v = Doc2Vec.load(f\"{MODEL_PATH}/{CATEGORY}_asin_d2v.model\")"
   ]
  },
  {
   "source": [
    "#### Testing retrieval of vectors via index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['A1KLRMWW2FWPL4'], ['A2G5TCU2WDFZ65'], ['A1RLQXYNCMWRWN'], ['A8U3FAMSJVHS5'], ['A3GEOILWLK86XM'], ['A27UF1MSF3DB2'], ['A16GFPNVF4Y816'], ['A2M2APVYIB2U6K'], ['A1NJ71X3YPQNQ9'], ['A3EERSWHAI6SO']]\n"
     ]
    }
   ],
   "source": [
    "print([i[1] for i in user_documents[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.02055808, -0.05843449, -0.05547184,  0.01778437,  0.00469217,\n",
       "       -0.03501041, -0.00673267,  0.03883101, -0.01805281, -0.0209298 ,\n",
       "       -0.03623148, -0.00159739, -0.05911888,  0.0729358 , -0.03065956,\n",
       "       -0.02815126,  0.02357766, -0.05407812, -0.00607294,  0.10883669,\n",
       "       -0.04466315,  0.00948019,  0.03358833, -0.01949031,  0.00525372,\n",
       "        0.03148691, -0.0259487 , -0.00393125, -0.03223359, -0.07157879,\n",
       "       -0.02014398,  0.02571197, -0.07092678, -0.01904643, -0.04474743,\n",
       "       -0.00384329,  0.04944458, -0.00731054,  0.03500834, -0.03514026,\n",
       "       -0.01279218,  0.02124795, -0.03646373, -0.04348272,  0.02164138,\n",
       "       -0.03481154,  0.03408755, -0.0179861 , -0.00462096,  0.07106731,\n",
       "       -0.03033135,  0.03183918, -0.0095906 , -0.0425368 , -0.05297974,\n",
       "        0.00681152,  0.01870618, -0.00066938,  0.02898959, -0.04136917,\n",
       "       -0.01876115, -0.02415633, -0.0366751 , -0.00385988,  0.07085671,\n",
       "       -0.00878479,  0.00941915, -0.04667619, -0.02240438, -0.07442102,\n",
       "        0.01172889,  0.04799885,  0.0224192 , -0.05843763, -0.0122609 ,\n",
       "        0.03335764, -0.00646947,  0.0023951 , -0.05808043,  0.05049259,\n",
       "        0.02869298, -0.07060311, -0.02819549,  0.08832964, -0.02580999,\n",
       "       -0.00254192, -0.00656417,  0.04744618,  0.05343859, -0.02516514,\n",
       "        0.06207803, -0.07590087, -0.03564372, -0.01069557, -0.02427637,\n",
       "        0.01033422,  0.03331715,  0.01101676, -0.04890436,  0.05566774,\n",
       "       -0.02885022, -0.00308489,  0.07566214,  0.00967525, -0.02385368,\n",
       "       -0.02467603, -0.02154524,  0.06820426, -0.07193049, -0.02553957,\n",
       "       -0.06308971, -0.01514923, -0.03906152, -0.02260614, -0.0179332 ,\n",
       "        0.02904478,  0.00158675,  0.00778539,  0.06140223, -0.04627376,\n",
       "        0.02459338,  0.03120343,  0.01207983, -0.04242843,  0.03824018,\n",
       "        0.00049249,  0.03099849, -0.03092429, -0.00832307, -0.03647054,\n",
       "        0.04475996,  0.06178118,  0.01224778, -0.05674658,  0.03799466,\n",
       "       -0.00423586, -0.0582651 ,  0.02113074, -0.01389883,  0.00814351,\n",
       "        0.02838524,  0.01728528,  0.06556802,  0.03415157,  0.07967327,\n",
       "        0.03106901, -0.07705415, -0.02151448,  0.10837466, -0.03139601],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "user_d2v.dv[0]"
   ]
  },
  {
   "source": [
    "#### Testing retrieval of vectors via tags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.02055808, -0.05843449, -0.05547184,  0.01778437,  0.00469217,\n",
       "       -0.03501041, -0.00673267,  0.03883101, -0.01805281, -0.0209298 ,\n",
       "       -0.03623148, -0.00159739, -0.05911888,  0.0729358 , -0.03065956,\n",
       "       -0.02815126,  0.02357766, -0.05407812, -0.00607294,  0.10883669,\n",
       "       -0.04466315,  0.00948019,  0.03358833, -0.01949031,  0.00525372,\n",
       "        0.03148691, -0.0259487 , -0.00393125, -0.03223359, -0.07157879,\n",
       "       -0.02014398,  0.02571197, -0.07092678, -0.01904643, -0.04474743,\n",
       "       -0.00384329,  0.04944458, -0.00731054,  0.03500834, -0.03514026,\n",
       "       -0.01279218,  0.02124795, -0.03646373, -0.04348272,  0.02164138,\n",
       "       -0.03481154,  0.03408755, -0.0179861 , -0.00462096,  0.07106731,\n",
       "       -0.03033135,  0.03183918, -0.0095906 , -0.0425368 , -0.05297974,\n",
       "        0.00681152,  0.01870618, -0.00066938,  0.02898959, -0.04136917,\n",
       "       -0.01876115, -0.02415633, -0.0366751 , -0.00385988,  0.07085671,\n",
       "       -0.00878479,  0.00941915, -0.04667619, -0.02240438, -0.07442102,\n",
       "        0.01172889,  0.04799885,  0.0224192 , -0.05843763, -0.0122609 ,\n",
       "        0.03335764, -0.00646947,  0.0023951 , -0.05808043,  0.05049259,\n",
       "        0.02869298, -0.07060311, -0.02819549,  0.08832964, -0.02580999,\n",
       "       -0.00254192, -0.00656417,  0.04744618,  0.05343859, -0.02516514,\n",
       "        0.06207803, -0.07590087, -0.03564372, -0.01069557, -0.02427637,\n",
       "        0.01033422,  0.03331715,  0.01101676, -0.04890436,  0.05566774,\n",
       "       -0.02885022, -0.00308489,  0.07566214,  0.00967525, -0.02385368,\n",
       "       -0.02467603, -0.02154524,  0.06820426, -0.07193049, -0.02553957,\n",
       "       -0.06308971, -0.01514923, -0.03906152, -0.02260614, -0.0179332 ,\n",
       "        0.02904478,  0.00158675,  0.00778539,  0.06140223, -0.04627376,\n",
       "        0.02459338,  0.03120343,  0.01207983, -0.04242843,  0.03824018,\n",
       "        0.00049249,  0.03099849, -0.03092429, -0.00832307, -0.03647054,\n",
       "        0.04475996,  0.06178118,  0.01224778, -0.05674658,  0.03799466,\n",
       "       -0.00423586, -0.0582651 ,  0.02113074, -0.01389883,  0.00814351,\n",
       "        0.02838524,  0.01728528,  0.06556802,  0.03415157,  0.07967327,\n",
       "        0.03106901, -0.07705415, -0.02151448,  0.10837466, -0.03139601],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "user_d2v.dv['A1KLRMWW2FWPL4']   # '2' is at index of 2 as shown previously"
   ]
  },
  {
   "source": [
    "### 3. Examining D2V Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.1 Are inferred vectors close to the precalculated ones?\n",
    "\n",
    "Example of detailed review with better results:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For doc 187116...\nTokens: ['add', 'bracelet', 'it', 'pretty', 'look', 'nice', 'it', 'bright', 'stand']\nMost similar D2V vectors: [('B007XE5TKM', 0.9632561206817627), ('B00FI9PPS0', 0.9579210877418518), ('B00CVP0IFY', 0.957432210445404)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.choice(list(train['index'].unique()), 1)\n",
    "doc_tokens = train[train['index'] == doc_id[0]]['tokenizedReviewText'].values[0]\n",
    "\n",
    "print(f\"For doc {doc_id[0]}...\\nTokens: {doc_tokens}\")\n",
    "inferred_docvec = asin_d2v.infer_vector(doc_tokens, steps=100, alpha=0.025)\n",
    "print(f'Most similar D2V vectors: {asin_d2v.dv.most_similar([inferred_docvec], topn=3)}')"
   ]
  },
  {
   "source": [
    "Example of less-detailed review with worse results:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = np.random.choice(list(train['index'].unique()), 1)\n",
    "doc_tokens = train[train['index'] == doc_id[0]]['tokenizedReviewText'].values[0]\n",
    "\n",
    "print(f\"For doc {doc_id[0]}...\\nTokens: {doc_tokens}\")\n",
    "inferred_docvec = model.infer_vector(doc_tokens, steps=100, alpha=0.025)\n",
    "print(f'Most similar D2V vectors: {model.dv.most_similar([inferred_docvec], topn=3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_tokens = train[train['index'] == 444194]['tokenizedReviewText'].values[0]\n",
    "print(similar_tokens)"
   ]
  },
  {
   "source": [
    "### (WIP) Creating appropriate mapping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Before we can generate recommendations based on the vectors trained by `D2V`, we will need to generate a few mappings to help us aggregate vectors on both users and items level.\n",
    "\n",
    "As our model ideates based on the how both `user` and `item` can be represented by their `documents`  which, in this case is an aggregation of all the past reviews. Hence, we will need to generate mappings that identifies:\n",
    "\n",
    "1. `User-Review`: What are the reviews that have been given by a user.\n",
    "2. `Product-Review`: What are the reviews that have been received by a product.\n",
    "\n",
    "With the above mapping, we are able to develop a item-level recommendation based on the past items purchased and reviewed by the users. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_review_map = train.groupby(['reviewerID'])['index'].progress_apply(list).to_dict()\n",
    "pprint(list(train_user_review_map.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD METHOD\n",
    "# train_user_index_map = {}\n",
    "# users = list(set(train['reviewerID']))\n",
    "# index = 0\n",
    "# for user in tqdm(users):\n",
    "#     if user not in train_user_index_map.values():\n",
    "#         train_user_index_map[index] = user\n",
    "#         index += 1\n",
    "#     else:\n",
    "#         pass\n",
    "# print(len(train_user_index_map))\n",
    "\n",
    "train_user_index_map = pd.DataFrame({'reviewerID': list(set(train['reviewerID']))}).to_dict()['reviewerID']\n",
    "print(len(train_user_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify checks\n",
    "print(train['reviewerID'].nunique())\n",
    "print(len(train_user_review_map))\n",
    "print(len(train_user_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod_review_map = train.groupby(['asin'])['index'].progress_apply(list).to_dict()\n",
    "pprint(list(train_prod_review_map.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD METHOD\n",
    "# train_prod_index_map = {}\n",
    "# prods = list(train_movie_reviews['asin'].unique())\n",
    "# index = 0\n",
    "# for prod in tqdm(prods):\n",
    "#     if prod not in train_prod_index_map.values():\n",
    "#         train_prod_index_map[index] = prod\n",
    "#         index += 1\n",
    "#     else:\n",
    "#         pass\n",
    "# print(len(train_prod_index_map))\n",
    "\n",
    "train_prod_index_map = pd.DataFrame({'asin': list(set(train['asin']))}).to_dict()['asin']\n",
    "pprint(list(train_prod_index_map.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify checks\n",
    "print(train['asin'].nunique())\n",
    "print(len(train_prod_review_map))\n",
    "print(len(train_prod_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_PATH = Path(\"data/processed/mappings/\")\n",
    "\n",
    "np.save(f'{MAP_PATH}/{CATEGORY}_train_user_review_map.npy', train_user_review_map)\n",
    "np.save(f'{MAP_PATH}/{CATEGORY}_train_user_index_map.npy', train_user_index_map)\n",
    "np.save(f'{MAP_PATH}/{CATEGORY}_train_prod_review_map.npy', train_prod_review_map)\n",
    "np.save(f'{MAP_PATH}/{CATEGORY}_train_prod_index_map.npy', train_prod_index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_PATH = Path(\"data/processed/mappings/\")\n",
    "\n",
    "train_user_review_map = np.load(f'{MAP_PATH}/{CATEGORY}_train_user_review_map.npy', allow_pickle=True).item()\n",
    "train_user_index_map = np.load(f'{MAP_PATH}/{CATEGORY}_train_user_index_map.npy', allow_pickle=True).item()\n",
    "train_prod_review_map = np.load(f'{MAP_PATH}/{CATEGORY}_train_prod_review_map.npy', allow_pickle=True).item()\n",
    "train_prod_index_map = np.load(f'{MAP_PATH}/{CATEGORY}_train_prod_index_map.npy', allow_pickle=True).item()"
   ]
  },
  {
   "source": [
    "### Creating aggregated vector by product `asin` and building ANN trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating approximate nearest neighbour setup\n",
    "f = 150\n",
    "t = AnnoyIndex(f, 'angular')    # using cosine measure\n",
    "\n",
    "agg_prod_vectors = {}\n",
    "# in range for all users\n",
    "for k, v in tqdm(train_prod_index_map.items()):\n",
    "    ind_reviews_vectors = np.zeros(f)\n",
    "    # retrieve the agg reviews list based on user id\n",
    "    # asin = train_prod_index_map[k]\n",
    "    agg_list = train_prod_review_map[v]\n",
    "    for j in agg_list:\n",
    "        # retrieve the row number\n",
    "        # row_num = temp.loc[temp['index'] == j].index[0]\n",
    "        # retrieve the vector from d2v\n",
    "        try:\n",
    "            ind_reviews_vectors += np.array(model.dv[str(j)])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # aggregate all doc vectors from a prod\n",
    "    agg_vectors = np.divide(ind_reviews_vectors, len(agg_list))\n",
    "    agg_prod_vectors[k] = agg_vectors\n",
    "    # we need to add these into the ann object\n",
    "    t.add_item(k, agg_vectors)\n",
    "\n",
    "# build the trees\n",
    "ANN_PATH = Path(f\"anns/{CATEGORY}/\")\n",
    "\n",
    "t.build(300)\n",
    "t.save(f'{ANN_PATH}/{CATEGORY}-f150.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 150\n",
    "\n",
    "ANN_PATH = Path(f\"anns/{CATEGORY}/\")\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load(f'{ANN_PATH}/{CATEGORY}-f150.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{MAP_PATH}/{CATEGORY}_agg_prod_vectors.npy', agg_prod_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_prod_vectors = np.load(f'{MAP_PATH}/{CATEGORY}_agg_prod_vectors.npy', allow_pickle=True).item()"
   ]
  },
  {
   "source": [
    "### Creating aggregated vector by users `reviewerID`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 150\n",
    "\n",
    "agg_user_vectors = {}\n",
    "# in range for all users\n",
    "for k, v in tqdm(train_user_index_map.items()):\n",
    "    ind_reviews_vectors = np.zeros(f)\n",
    "    # retrieve the agg reviews list based on user id\n",
    "    # user_id = train_user_index_map[i]\n",
    "    agg_list = train_user_review_map[v]\n",
    "    for j in agg_list:\n",
    "        # retrieve the vector from d2v\n",
    "        ind_reviews_vectors += np.array(model.dv[str(j)])\n",
    "    # aggregate all doc vectors from a prod\n",
    "    agg_vectors = np.divide(ind_reviews_vectors, len(agg_list))\n",
    "    agg_user_vectors[k] = agg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{MAP_PATH}/{CATEGORY}_agg_user_vectors.npy', agg_user_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_user_vectors = np.load(f'{MAP_PATH}/{CATEGORY}_agg_user_vectors.npy', allow_pickle=True).item()"
   ]
  },
  {
   "source": [
    "### Using Nearest Neighbor Search (NNS) to retrieve top 10 items "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Extending the idea of generating recommendations, we able to generate recommendations based on two factors:\n",
    "\n",
    "1. Past users's purchase (using their reviews)\n",
    "2. Item-level similarities based on aggregation of reviews embeddings\n",
    "\n",
    "This meant that, given a user's past purchase history, where we generated an aggregation of reviews embeddings, we are able to use that same vector to help recommend items from the item *f*-dimensional space. Also, based on an instance where perhaps last click on a certain item, we are able to generate recommendations based on the comparison of similarities reviews from the clicked items to the other items within the item *f*-dimensional space."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Using users-level recommendation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_user_recommendations():\n",
    "    user_index = np.random.randint(0, len(train_user_index_map))\n",
    "    reviewer_agg_vectors = agg_user_vectors[user_index]\n",
    "    reviewer_id = train_user_index_map[user_index]\n",
    "\n",
    "    ann_similar_items = u.get_nns_by_vector(reviewer_agg_vectors, 25, search_k=-1, include_distances=False)\n",
    "\n",
    "    recommendations = pd.DataFrame(columns=['rank', 'asin', 'overall', 'review_counts', 'titles', 'brands'])\n",
    "    ranks = [i for i in range(1,11,1)]\n",
    "    asins = []\n",
    "    overalls = []\n",
    "    review_counts = []\n",
    "    titles = []\n",
    "    brands = []\n",
    "    for item in ann_similar_items:\n",
    "        user_history_train = train[train['reviewerID'] == reviewer_id]['asin'].unique()\n",
    "        if item not in user_history_train:\n",
    "            asin = train_prod_index_map[item]\n",
    "            overall = train[train['asin'] == asin]['overall'].mean()\n",
    "            review_count = len(train[train['asin'] == asin]['reviewText'])\n",
    "            title = train[train['asin'] == asin]['title'].unique()[0]\n",
    "            brand = train[train['asin'] == asin]['brand'].unique()[0]\n",
    "\n",
    "            asins.append(asin)\n",
    "            overalls.append(overall)\n",
    "            review_counts.append(review_count)\n",
    "            titles.append(title if len(title) > 0 else '')\n",
    "            brands.append(brand if len(str(brand)) > 0 else '')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    recommendations['rank'] = ranks\n",
    "    recommendations['asin'] = asins[:10]\n",
    "    recommendations['overall'] = overalls[:10]\n",
    "    recommendations['review_counts'] = review_counts[:10]\n",
    "    recommendations['titles'] = titles[:10]\n",
    "    recommendations['brands'] = brands[:10]\n",
    "    \n",
    "    # retrieving the recommendations\n",
    "    print(f\"For user: {reviewer_id}\\n\")\n",
    "    print(f\"Recommended items are:\")\n",
    "    for index, row in recommendations.iterrows():\n",
    "        print(f\"Rank: {row[0]} | {row[1]} | Rating: {row[2]:.2f} | Reviewed: {row[3]} | Title: {row[4]}\")\n",
    "\n",
    "    return reviewer_id\n",
    "\n",
    "reviewer_id = generate_random_user_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the past purchase history of the user\n",
    "user_history_train= train[train['reviewerID'] == reviewer_id]\n",
    "user_history_train[['title', 'asin', 'overall', 'reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_test = test[test['reviewerID'] == reviewer_id]\n",
    "user_history_test[['title', 'asin', 'overall', 'reviewText']]"
   ]
  },
  {
   "source": [
    "#### Using items-level recommendation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_prod_recommendations():\n",
    "    user_index = np.random.randint(0, len(train_user_index_map))\n",
    "    reviewer_agg_vectors = agg_user_vectors[user_index]\n",
    "    reviewer_id = train_user_index_map[user_index]\n",
    "\n",
    "    prod_index = np.random.randint(0, len(train_prod_index_map))\n",
    "    prod_agg_vectors = agg_prod_vectors[prod_index]\n",
    "    prod_asin = train_prod_index_map[prod_index]\n",
    "    prod_title = train[train['asin'] == prod_asin]['title'].unique()[0]\n",
    "\n",
    "    ann_similar_items = u.get_nns_by_vector(prod_agg_vectors, 50, search_k=-1, include_distances=False)\n",
    "\n",
    "    recommendations = pd.DataFrame(columns=['rank', 'asin', 'overall', 'review_counts', 'titles', 'brands'])\n",
    "    ranks = [i for i in range(1,12,1)]\n",
    "    asins = []\n",
    "    overalls = []\n",
    "    review_counts = []\n",
    "    titles = []\n",
    "    brands = []\n",
    "    for item in ann_similar_items:\n",
    "        user_history_train = train[train['reviewerID'] == reviewer_id]['asin'].unique()\n",
    "        if item not in user_history_train:\n",
    "            asin = train_prod_index_map[item]\n",
    "            overall = train[train['asin'] == asin]['overall'].mean()\n",
    "            review_count = len(train[train['asin'] == asin]['reviewText'])\n",
    "            title = train[train['asin'] == asin]['title'].unique()[0]\n",
    "            brand = train[train['asin'] == asin]['brand'].unique()[0]\n",
    "\n",
    "            asins.append(asin)\n",
    "            overalls.append(overall)\n",
    "            review_counts.append(review_count)\n",
    "            titles.append(title if len(title) > 0 else '')\n",
    "            brands.append(brand if len(str(brand)) > 0 else '')\n",
    "\n",
    "    recommendations['rank'] = ranks\n",
    "    recommendations['asin'] = asins[:11]\n",
    "    recommendations['overall'] = overalls[:11]\n",
    "    recommendations['review_counts'] = review_counts[:11]\n",
    "    recommendations['titles'] = titles[:11]\n",
    "    recommendations['brands'] = brands[:11]\n",
    "    \n",
    "    # retrieving the recommendations\n",
    "    print(f\"For user: {reviewer_id}, given last viewed/clicked product: {prod_asin}  {prod_title}...\\n\")\n",
    "    print(f\"Recommended items are:\")\n",
    "    for index, row in recommendations.iterrows():\n",
    "        print(f\"Rank: {row[0]} | {row[1]} | Rating: {row[2]:.2f} | Reviewed: {row[3]} | Title: {row[4]}\")\n",
    "\n",
    "generate_random_prod_recommendations()"
   ]
  },
  {
   "source": [
    "### Computing Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}